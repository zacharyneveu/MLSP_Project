A subset of $2000$ frames from the MAESTRO dataset\cite{hawthorne_enabling_2019} is used to fit the dictionary. This data consists of audio files of piano music, alongside \ac{MIDI} files which contain the times, frequencies, and amplitudes of each note played. These audio files are oversampled to $44,100\,Hz$  to cover the full range of human hearing, however this is  generally unnecessary for music transcription (the highest piano note has an $F_0$ of $4186\,Hz$), so the audio files are downsampled by a factor of $4$ to $11,025\,Hz$. Using the \ac{MIDI} annotations, audio files are then split at note onsets. Each frame is then zero-padded or cropped so that the length of all frames is identical and of a tractable size of $512$ samples. These frames were then concatenated into a $2000\times 512$ matrix suitable for training the dictionary.
