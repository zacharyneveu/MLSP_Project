A subset of the MAESTRO dataset is used to fit the dictionary \cite{hawthorne_enabling_2019}. This data consists of audio files of piano music alongside \ac{MIDI} files which contain the times, frequencies, and amplitudes of each note played. These audio files are oversampled to $44,100\,Hz$  to cover the full range of human hearing, however this is  generally unnecessary for music transcription (the highest piano note has an $F_0$ of $4186\,Hz$), so the audio files were downsampled by a factor of $4$ to $11,025\,Hz$. Using the \ac{MIDI} annotations, audio files were then split at note onsets. Each frame was then zero-padded or cropped so that the length of all frames was identical and of a tractable size of $512$ samples. These frames were then concatenated into a $N\times 512$ matrix suitable for training the dictionary.
